{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf4538d-15f7-44de-ab7a-32048746a63b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# xbatcher for Machine Learning Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4312093a-0564-4134-8cb5-0e23515709e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Here, we will be covering how to use xbatcher with Keras/Tensorflow convolutional neural network (CNN) models. \n",
    "\n",
    "Prerequisites:\n",
    "\n",
    "| Concepts | Importance | Notes |\n",
    "| --- | --- | --- |\n",
    "| [Xarray](https://foundations.projectpythia.org/core/xarray/xarray-intro.html) | Necessary | |\n",
    "| Keras/Tensorflow | Strongly Recommended | Not strictly needed to understand this tutorial |\n",
    "\n",
    "This notebook replicates the work of [Sinha and Abernathey, 2021](https://www.frontiersin.org/article/10.3389/fmars.2021.672477), where the goal is to use a CNN to learn ocean surface currents (which are usually inferred diagnostically or modelled) from variables that can be observed directly, like sea surface temperature (SST) or wind stress. \n",
    "\n",
    "Can we learn to predict ocean currents with just one snapshot of data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79054b8e-90c1-4ea4-868e-dfc28cfcdbf9",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4011f51-74bb-4d47-8249-73a613e2af3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7094a43-9c92-4b88-80ac-9a7546e49508",
   "metadata": {},
   "source": [
    "To start, let's import some libraries we'll need. The important libraries here are `numpy`, `xarray`, `xbatcher` and `tensorflow`, while most of the others aren't strictly necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e48a969-b28e-4fc9-864a-8a402ae82252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterable\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7edff03e-636b-4c08-85d4-2f25a9507011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac258fca-23fa-4b04-91d5-17e9a7606ff6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xbatcher as xb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6191414-f0e2-4731-bbf3-e7976e015c44",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8b4eb-665f-47f4-ada9-ae6c92c8cd21",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Designing Scenarios\n",
    "\n",
    "We want to experiment with different neural network models by providing different inputs, and perhaps by playing with whether or not we run them through a convolutional layer. There are a lot of possibilities here, and if we approach it haphazardly, we'll end up with a mess of scattered experiments and results mixed in with other code. \n",
    "\n",
    "Instead, we can be more systematic about it. We know we want to define an individual scenario once, and then have it stay constant through the workflow. This way, there will be no complexities later on about whether we're referring to the right dataset, etc. With that in mind, we should use a `dataclass`. We want something minimal here, just enough to store the names of variables we're interested in. \n",
    "\n",
    "What is the structure of each experiment? We want some input variables to be run through a 2D convolutional layer, while some other inputs will be passed through directly to the dense part of the neural network. Both of these can be lists of strings, so we define `conv_var` and `input_var` as `Iterable[str]`.\n",
    "\n",
    "Likewise, we have more than one target, so we define the `target` item as `Iterable[str]` as well. Outside of the `Scenario` dataclass, we define `target` as a list: `['U', 'V']`. Since we're only interested in learning the currents, this won't change.\n",
    "\n",
    "Finally, we need to name each scenario something distinct, so when we create data subsets for training, testing, and prediction, we can recover them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7c7fca5-9d89-4749-9c29-27461aed0d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Scenario:\n",
    "    conv_var: Iterable[str]\n",
    "    input_var: Iterable[str]\n",
    "    target: Iterable[str]\n",
    "    name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaad7588-a1c5-42ff-8835-5f677f11937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['U', 'V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb48d30-7cfe-4d46-b510-78af260b810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc1 = Scenario(['SSH'],             ['TAUX', 'TAUY'], target, name = \"derp\")\n",
    "sc5 = Scenario(['SSH', 'SST'], ['X', 'TAUX', 'TAUY'], target, name = \"herp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd17f20-8e09-43f2-9fbb-298043cd6c9f",
   "metadata": {},
   "source": [
    "### Data and Preprocessing\n",
    "\n",
    "For our dataset, we will be using ocean data from a high-resolution CESM POP model. \n",
    "\n",
    "We have some necessary I/O routines, but they aren't central to our problem, aside from the addtion of the new variables `X`, `Y`, `Z`, `dx` and `dy`, which represent Euclidean positions and distances between grid points.\n",
    "\n",
    "You can have a look in the notebook below if you're curious about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd7dfb91-ca1d-4a4d-85ed-af12c32e8776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run ./surface_currents_prep.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c198bde4-7683-4f47-ad7d-09cdcf55bdb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "From this notebook, we get a few new functions.\n",
    "\n",
    "  * `prepare_data` takes a scenario, as well as the time slices for training, testing, and prediction we are interested in, and the time slice we'll use for the NaN mask. It adds the new grid variables, and then stores each slice in a new zarr store that we can access later. This speeds up future I/O, which is helpful when modifying the model. Each scenario is stored separately.\n",
    "  * `load_training_data` loads the training data created for the scenario passed to it.\n",
    "  * `load_test_data` loads the testing data created for the scenario passed to it.\n",
    "  * `load_predict_data` loads the prediction input data created for the scenario passed to it.\n",
    "  \n",
    "You can comment out `prepare_data` after you've run it once, it will save time if you rerun the whole notebook again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf745763-351f-415f-8909-433d06eb8356",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bad Request: https://storage.googleapis.com/download/storage/v1/b/pangeo-cesm-pop/o/control%2F.zmetadata?alt=media\nUser project specified in the request is invalid.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/folders/dd/_xm_pbpd3flgbvbnt7qhd70snnbpj_/T/ipykernel_48687/3337232756.py:3\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(sc, training_time, test_time, predict_time, mask_time)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_data\u001b[39m(sc, training_time, test_time, predict_time, mask_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m      2\u001b[0m     cat \u001b[38;5;241m=\u001b[39m open_catalog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/pangeo-data/pangeo-datastore/master/intake-catalogs/ocean/CESM_POP.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     ds  \u001b[38;5;241m=\u001b[39m \u001b[43mcat\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCESM_POP_hires_control\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mrename({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mU1_1\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV1_1\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTAUX_2\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTAUX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTAUY_2\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTAUY\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSSH_2\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSSH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mULONG\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mULAT\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYU\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m      5\u001b[0m     ds \u001b[38;5;241m=\u001b[39m add_grid(ds)\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/intake_xarray/base.py:69\u001b[0m, in \u001b[0;36mDataSourceMixin.to_dask\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dask\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return xarray object where variables are dask arrays\"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/intake_xarray/base.py:44\u001b[0m, in \u001b[0;36mDataSourceMixin.read_chunked\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_chunked\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return xarray object (which will have chunks)\"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/intake/source/base.py:84\u001b[0m, in \u001b[0;36mDataSourceBase._load_metadata\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"load metadata only if needed\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/intake_xarray/base.py:18\u001b[0m, in \u001b[0;36mDataSourceMixin._get_schema\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_cache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlpath)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdims\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds\u001b[38;5;241m.\u001b[39mdims),\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_vars\u001b[39m\u001b[38;5;124m'\u001b[39m: {k: \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds[k]\u001b[38;5;241m.\u001b[39mcoords)\n\u001b[1;32m     23\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds\u001b[38;5;241m.\u001b[39mdata_vars\u001b[38;5;241m.\u001b[39mkeys()},\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoords\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds\u001b[38;5;241m.\u001b[39mcoords\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[1;32m     25\u001b[0m     }\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon_server\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/intake_xarray/xzarr.py:46\u001b[0m, in \u001b[0;36mZarrSource._open_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_mfdataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlpath, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/xarray/backends/api.py:573\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    562\u001b[0m     decode_cf,\n\u001b[1;32m    563\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    570\u001b[0m )\n\u001b[1;32m    572\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 573\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    580\u001b[0m     backend_ds,\n\u001b[1;32m    581\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    592\u001b[0m )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/xarray/backends/zarr.py:1029\u001b[0m, in \u001b[0;36mZarrBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, synchronizer, consolidated, chunk_store, storage_options, stacklevel, zarr_version)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1010\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1026\u001b[0m     zarr_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1027\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m   1028\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m-> 1029\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mZarrStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynchronizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsolidated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconsolidate_on_close\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunk_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunk_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacklevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzarr_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzarr_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/xarray/backends/zarr.py:469\u001b[0m, in \u001b[0;36mZarrStore.open_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close, chunk_store, storage_options, append_dim, write_region, safe_chunks, stacklevel, zarr_version, write_empty)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m consolidated:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# TODO: an option to pass the metadata_key keyword\u001b[39;00m\n\u001b[0;32m--> 469\u001b[0m     zarr_group \u001b[38;5;241m=\u001b[39m \u001b[43mzarr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_consolidated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    471\u001b[0m     zarr_group \u001b[38;5;241m=\u001b[39m zarr\u001b[38;5;241m.\u001b[39mopen_group(store, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/zarr/convenience.py:1335\u001b[0m, in \u001b[0;36mopen_consolidated\u001b[0;34m(store, metadata_key, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1332\u001b[0m         metadata_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta/root/consolidated/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m metadata_key\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;66;03m# setup metadata store\u001b[39;00m\n\u001b[0;32m-> 1335\u001b[0m meta_store \u001b[38;5;241m=\u001b[39m \u001b[43mConsolidatedStoreClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;66;03m# pass through\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m chunk_store \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_store\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m store\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/zarr/storage.py:2962\u001b[0m, in \u001b[0;36mConsolidatedMetadataStore.__init__\u001b[0;34m(self, store, metadata_key)\u001b[0m\n\u001b[1;32m   2959\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore \u001b[38;5;241m=\u001b[39m Store\u001b[38;5;241m.\u001b[39m_ensure_store(store)\n\u001b[1;32m   2961\u001b[0m \u001b[38;5;66;03m# retrieve consolidated metadata\u001b[39;00m\n\u001b[0;32m-> 2962\u001b[0m meta \u001b[38;5;241m=\u001b[39m json_loads(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetadata_key\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;66;03m# check format of consolidated metadata\u001b[39;00m\n\u001b[1;32m   2965\u001b[0m consolidated_format \u001b[38;5;241m=\u001b[39m meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzarr_consolidated_format\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/zarr/storage.py:1429\u001b[0m, in \u001b[0;36mFSStore.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1427\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_key(key)\n\u001b[1;32m   1428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1429\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1431\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/fsspec/mapping.py:155\u001b[0m, in \u001b[0;36mFSMap.__getitem__\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    153\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key_to_str(key)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing_exceptions:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/fsspec/asyn.py:103\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/fsspec/asyn.py:56\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     54\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     58\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/fsspec/asyn.py:461\u001b[0m, in \u001b[0;36mAsyncFileSystem._cat\u001b[0;34m(self, path, recursive, on_error, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m     ex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(is_exception, out), \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ex:\n\u001b[0;32m--> 461\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ex\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28mlen\u001b[39m(paths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m paths[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_protocol(path)\n\u001b[1;32m    466\u001b[0m ):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    468\u001b[0m         k: v\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(paths, out)\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m on_error \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124momit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_exception(v)\n\u001b[1;32m    471\u001b[0m     }\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/fsspec/asyn.py:245\u001b[0m, in \u001b[0;36m_run_coros_in_chunks.<locals>._run_coro\u001b[0;34m(coro, i)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_coro\u001b[39m(coro, i):\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout), i\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_exceptions:\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/asyncio/tasks.py:452\u001b[0m, in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m loop \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mget_running_loop()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    455\u001b[0m     fut \u001b[38;5;241m=\u001b[39m ensure_future(fut, loop\u001b[38;5;241m=\u001b[39mloop)\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/gcsfs/core.py:1060\u001b[0m, in \u001b[0;36mGCSFileSystem._cat_file\u001b[0;34m(self, path, start, end, **kwargs)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1059\u001b[0m     head \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1060\u001b[0m headers, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, u2, headers\u001b[38;5;241m=\u001b[39mhead)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/gcsfs/core.py:445\u001b[0m, in \u001b[0;36mGCSFileSystem._call\u001b[0;34m(self, method, path, json_out, info_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, path, \u001b[38;5;241m*\u001b[39margs, json_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, info_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    443\u001b[0m ):\n\u001b[1;32m    444\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 445\u001b[0m     status, headers, info, contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    446\u001b[0m         method, path, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    447\u001b[0m     )\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m json_out:\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m json\u001b[38;5;241m.\u001b[39mloads(contents)\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/decorator.py:221\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    220\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/gcsfs/retry.py:126\u001b[0m, in \u001b[0;36mretry_request\u001b[0;34m(func, retries, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retry \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mmin\u001b[39m(random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m (retry \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m32\u001b[39m))\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    128\u001b[0m     HttpError,\n\u001b[1;32m    129\u001b[0m     requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m     aiohttp\u001b[38;5;241m.\u001b[39mclient_exceptions\u001b[38;5;241m.\u001b[39mClientError,\n\u001b[1;32m    133\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(e, HttpError)\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequester pays\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m    138\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/gcsfs/core.py:438\u001b[0m, in \u001b[0;36mGCSFileSystem._request\u001b[0;34m(self, method, path, headers, json, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m info \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mrequest_info  \u001b[38;5;66;03m# for debug only\u001b[39;00m\n\u001b[1;32m    436\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m r\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 438\u001b[0m \u001b[43mvalidate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m status, headers, info, contents\n",
      "File \u001b[0;32m~/miniconda3/envs/xbatcher-ML-1-cookbook-dev/lib/python3.11/site-packages/gcsfs/retry.py:111\u001b[0m, in \u001b[0;36mvalidate_response\u001b[0;34m(status, content, path, args)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mProxyError()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(msg):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBad Request: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(error)\n",
      "\u001b[0;31mValueError\u001b[0m: Bad Request: https://storage.googleapis.com/download/storage/v1/b/pangeo-cesm-pop/o/control%2F.zmetadata?alt=media\nUser project specified in the request is invalid."
     ]
    }
   ],
   "source": [
    "prepare_data(sc5, 200, 1000, 1000, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ef4df4-6381-4f72-9589-26fdf584df67",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, we'll load our training data and pick out the part we want to train with. \n",
    "\n",
    "NOTE: Coordinates and attributes are dropped for speed, doing this shouldn't be necessary in future (optimized) versions of xarray/xbatcher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa848723-93ad-4aac-ae7a-ab26c535d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_training = load_training_data(sc5)\n",
    "ds_training = just_the_data(ds_training)\n",
    "ds_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b968f8-8da6-4163-b662-0436c82944d7",
   "metadata": {},
   "source": [
    "Looking inside `ds_training`, we see only the variables we would expect from `sc5`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977b1fd-12de-443c-b151-8cbd69e0b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_training = select_from(ds_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6bd52c-2b46-4379-8126-a26c33911f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.plot.contourf(ds_training['SST'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8847185d-5969-40cd-a4d1-ec2ff7892f83",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Setup\n",
    "\n",
    "We have a model architecuture we're happy with already defined, so for this tutorial, we'll focus on how to use xbatcher to generate training sets for the model. From the notebook below, we recieve:\n",
    "  * `get_model()` Creates a mixed neural network based on some parameters. The architecture is intentionally a little arbitrary in terms of the depth of the dense part of the network, the depth of the convolutional part of the network, and the convolution kernel size. Returns a compiled Keras model.\n",
    "  * `LossHistory()` Only needed here because it has to be passed to `model.fit()`.\n",
    "  * `train()` We will walk through this routine below.\n",
    "\n",
    "Have a look inside for more details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb9cda-ecd3-4ba8-919b-e187dc449839",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./surface_currents_model.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68406a0-1416-4176-9351-db955bffa0c1",
   "metadata": {},
   "source": [
    "Now the fun part: we define the `train` function to deal with high-level aspects of training the model, which means this is a good place to use xbatcher. Let's walk through it...\n",
    "\n",
    "The arguments to `train` are\n",
    "  * `ds: xr.DataSet` The dataset you want to work with.\n",
    "  * `sc: Scenario` The scenario you want to work with.\n",
    "  * `conv_dims: List[int]` This is the shape of the stencil that will be passed to the first convolutional layer. We are only interested in 2D convolutions here, so it will need to be a list of two integers. Note that this is distinct from the convolutional kernel.\n",
    "  * `nfilters: int` How many filters do we want to map the first convolution layer to?\n",
    "  * `conv_kernels: List[int]` Each entry denotes the convolution kernel of a new convolution layer. `train` works best for odd-numbered convolution kernels.\n",
    "  * `dense_layers: int` The number of dense layers in the model.\n",
    "  \n",
    "For this example, we only use one convolution layer, which makes some things simpler. Feel free to experiment with these parameters to use different data sets and create new CNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c3f985-8d31-4a99-9f65-1a37629ed606",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = sc5\n",
    "conv_dims = [5,5]\n",
    "nfilters = 80\n",
    "conv_kernels = [5]\n",
    "dense_layers = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36986b46-2a45-48b8-aa38-3e8062d7c502",
   "metadata": {},
   "source": [
    "We'll need some info about how to rectify the output of the convolution layers with raw input from other variables (see the surface_currents_model.ipynb notebook for more info). Based on the convolution kernel, we know how the output of a convolution layer will be shaped compared to the input: a halo of a certain size will be removed from the edges. For odd convolution kernels, the halo thickness is always $\\frac{n - 1}{2}$ where $n$ is the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd049f5b-2734-4f96-b595-20022ab58773",
   "metadata": {},
   "outputs": [],
   "source": [
    "halo_size = int((np.sum(conv_kernels) - len(conv_kernels))/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2651494b-8cc5-4777-9bde-bdd030b86c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "halo_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4716830e-d5a5-41ff-8fa9-ad862086bd40",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training a Model with xbatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3166b5-5e33-454a-8f2d-0647398a0a64",
   "metadata": {},
   "source": [
    "Since we are trying to learn from a single 2D snapshot, it makes sense to iterate in both latitude and longitude. What we want are individual samples of the size given by `conv_dims`, but batched in a way that we can pass the correct number of samples to the model as a single tensor. So, `input_dims` will contain entries for both `nlon` and `nlat`. To take full advantage of the available data, we can add an overlap to make sure halo points are fully included in the neighboring samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1deeb41-1256-4f60-a5e2-26ba6129c59c",
   "metadata": {},
   "source": [
    "NOTE: xbatcher currently runs slowly with `concat_input_dims=True`, and running without it will result in batches of size one. Therefore, we use an implemenatation of xarray rolling to mimic what xbatcher does. This is not good strategy when using large datasets, but for this example, the differences are minimal. We anticipate that fixed-size batches and some optimizations will be implemented in xbatcher in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee80ff17-fb53-428c-80a4-002565ee9b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlons, nlats = conv_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f0f36-c35a-4c43-aaed-2f9465f494a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bgen = xb.BatchGenerator(\n",
    "#     ds_training,\n",
    "#     {'nlon':nlons,       'nlat':nlats},\n",
    "#     {'nlon':2*halo_size, 'nlat':2*halo_size}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ce5189-eaa6-45bd-b9c1-098c5f131453",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlen = len(ds_training['nlat'])\n",
    "lonlen = len(ds_training['nlon'])\n",
    "nlon_range = range(nlons,lonlen,nlons - 2*halo_size)\n",
    "nlat_range = range(nlats,latlen,nlats - 2*halo_size)\n",
    "\n",
    "batch = (\n",
    "    ds_training\n",
    "    .rolling({\"nlat\": nlats, \"nlon\": nlons})\n",
    "    .construct({\"nlat\": \"nlat_input\", \"nlon\": \"nlon_input\"})[{'nlat':nlat_range, 'nlon':nlon_range}]\n",
    "    .stack({\"input_batch\": (\"nlat\", \"nlon\")}, create_index=False)\n",
    "    .rename_dims({'nlat_input':'nlat', 'nlon_input':'nlon'})\n",
    "    .transpose('input_batch',...)\n",
    "    # .chunk({'input_batch':32, 'nlat':nlats, 'nlon':nlons})\n",
    "    .dropna('input_batch')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f1a698-b0c7-4a21-8aea-d2c57657877e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnds = list(range(len(batch['input_batch'])))\n",
    "np.random.shuffle(rnds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4fd97c-db67-4f9d-bcb1-e69346842eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch[{'input_batch':(rnds)}]\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2944159-14af-469b-a4a5-10ac404d32ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use with rolling\n",
    "def batch_generator(batch_set, batch_size):\n",
    "    n = 0\n",
    "    while n < len(batch_set['input_batch']) - batch_size:\n",
    "        yield batch_set.isel({'input_batch':range(n,(n+batch_size))})\n",
    "        n += batch_size\n",
    "        \n",
    "# # use with xbatcher\n",
    "# def batch_generator(bgen, batch_size):\n",
    "#     b = (batch for batch in bgen)\n",
    "#     n = 0\n",
    "#     while n < 400:\n",
    "#         batch_stack = [ next(b) for i in range(batch_size) ]\n",
    "#         yield xr.concat(batch_stack, 'sample')\n",
    "#         n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef8a55f-8c7e-42ae-a846-5e0a8d8bc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgen = batch_generator(batch, 4096)\n",
    "# bgen = batch_generator(bgen, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6358d-90d1-43d6-941a-bf35ebb7b34b",
   "metadata": {},
   "source": [
    "We need a subsetting stencil (`sub`) to compensate for the fact that a halo is removed by each convolution layer. This means that the input_var variables will be the wrong size at the concat layer unless we strip the halo from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcfdcf2-7a88-4951-867e-2c37e8269f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = {'nlon':range(halo_size,nlons-halo_size),\n",
    "       'nlat':range(halo_size,nlats-halo_size)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bd2a8e-cf75-469b-be79-a53576ffc61b",
   "metadata": {},
   "source": [
    "Here, we generate our model and our history callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152667f-385c-4258-939f-e7c51544ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(halo_size, ds_training, sc, conv_dims, nfilters, conv_kernels, dense_layers)\n",
    "history = LossHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597dc73-a90e-482d-8580-63a761594c82",
   "metadata": {},
   "source": [
    "And now, we can construct our training loop. Most use cases of the `xb.BatchGenerator` will take the form of a for-loop with the construct `for batch in bgen`.\n",
    "\n",
    "Once we have a batch, we still have some things to do before we can pass the data to the model.\n",
    "\n",
    "So when we look at the contents of each batch, we see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69733cc-7a8f-41b3-9629-1fde72921b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = []\n",
    "# for batch in bgen:\n",
    "#     a = batch\n",
    "#     break\n",
    "# a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146f2c9-a123-4eec-873d-7f850fd13189",
   "metadata": {},
   "source": [
    "...but our model expects tensors where the different variables are stacked in a new dimension we will call `var`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c07ce1-66d1-413e-974e-fbc065950fb0",
   "metadata": {},
   "source": [
    "Looking at `model.fit()`, we have two separate inputs because of the distinction between convolved inputs and raw inputs. Therefore, the model expects these inputs to be given as a list of the two. The training target is relatively straightforward. On the next line, we have a couple of parameters we can experiment with. The important thing to note is the `batch_size` parameter; you may need to check that the sample dimension is compatible with the dimensions that `xb.BatchGenerator` returned. And finally, we pass our history class as a callback so we can see how the model training is progressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07cdac5-1d39-4ee7-b6f3-8bcd1a07d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in bgen:\n",
    "    \n",
    "    batch_conv   = [batch[x] for x in sc.conv_var]\n",
    "    batch_input  = [batch[x][sub] for x in sc.input_var]\n",
    "    batch_target = [batch[x][sub] for x in sc.target]\n",
    "    batch_conv   = xr.merge(batch_conv).to_array('var').transpose(...,'var')\n",
    "    batch_input  = xr.merge(batch_input).to_array('var').transpose(...,'var')\n",
    "    batch_target = xr.merge(batch_target).to_array('var').transpose(...,'var')\n",
    "\n",
    "    #clear_output(wait=True)\n",
    "    model.fit([batch_conv, batch_input],\n",
    "              batch_target,\n",
    "              batch_size=32, verbose=0,# epochs=4,\n",
    "              callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22219b0-65fb-427a-947b-b3ce050ca7fd",
   "metadata": {},
   "source": [
    "And now that we have our model trained, we can save it for future use. Note that once this model is saved, we don't need to rerun much from above to continue with testing or prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ec848-1f94-4551-96fc-f4ef5447a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/'+ sc.name)\n",
    "np.savez('models/history_'+sc.name, losses=history.mae, mse=history.mse, accuracy=history.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a999873e-8009-4568-b07b-c78f588d1b08",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364c5a4-1931-4a93-b315-2fe9d85f32e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(ds_training, sc5, conv_dims, conv_kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af828260-9b72-4204-b572-78a5dc313e64",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61008150-4074-44d6-a331-70bb17f87f55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d0ced-35f5-4a66-af18-7b6e2aab188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = load_test_data(sc5)\n",
    "ds_test = just_the_data(ds_test)\n",
    "ds_test = select_from(ds_test)\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e395aa55-21f8-48b3-962b-223bcd5b46c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlen = len(ds_test['nlat'])\n",
    "lonlen = len(ds_test['nlon'])\n",
    "nlon_range = range(nlons,lonlen,nlons - 2*halo_size)\n",
    "nlat_range = range(nlats,latlen,nlats - 2*halo_size)\n",
    "\n",
    "batch_test = (\n",
    "    ds_test\n",
    "    .rolling({\"nlat\": nlats, \"nlon\": nlons})\n",
    "    .construct({\"nlat\": \"nlat_input\", \"nlon\": \"nlon_input\"})[{'nlat':nlat_range, 'nlon':nlon_range}]\n",
    "    .stack({\"input_batch\": (\"nlat\", \"nlon\")}, create_index=False)\n",
    "    .rename_dims({'nlat_input':'nlat', 'nlon_input':'nlon'})\n",
    "    .transpose('input_batch',...)\n",
    "    # .chunk({'input_batch':32, 'nlat':nlats, 'nlon':nlons})\n",
    "    .dropna('input_batch')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb00a3b0-ab5a-4ff6-8617-7aa9d184a080",
   "metadata": {},
   "source": [
    "Let's load the trained model from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bd5d2f-d941-4770-a79c-4730f49263e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('models/'+ sc.name, custom_objects={'Grid_MAE':Grid_MAE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b49441-14c3-4299-b64a-84718e0c4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_conv   = [batch_test[x]      for x in sc.conv_var]\n",
    "test_input  = [batch_test[x][sub] for x in sc.input_var]\n",
    "test_target = [batch_test[x][sub] for x in sc.target]\n",
    "test_conv   = xr.merge(test_conv  ).to_array('var').transpose(...,'var')\n",
    "test_input  = xr.merge(test_input ).to_array('var').transpose(...,'var')\n",
    "test_target = xr.merge(test_target).to_array('var').transpose(...,'var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf4c7ca-ee04-47f4-b078-3b8bb6f15222",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([test_conv, test_input], test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc340ef2-2770-4860-9dbe-a7a88c6cf666",
   "metadata": {},
   "source": [
    "## Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f01f460-f5a3-4634-b9e7-fe276d178b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test(ds_test, sc5, conv_dims, conv_kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e1c23f-a215-4356-8700-b20175983c5a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e5dbcd-3447-47dc-aefa-9acba7031098",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2121aa99-f0a0-4303-98c3-ef725ff43065",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_predict = load_predict_data(sc5)\n",
    "ds_predict = just_the_data(ds_predict)\n",
    "ds_predict = select_from(ds_predict)\n",
    "ds_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82873940-6c94-406a-8b77-7d182ee63fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "latlen = len(ds_predict['nlat'])\n",
    "lonlen = len(ds_predict['nlon'])\n",
    "nlon_range = range(nlons,lonlen,nlons - 2*halo_size)\n",
    "nlat_range = range(nlats,latlen,nlats - 2*halo_size)\n",
    "\n",
    "batch_predict = (\n",
    "    ds_predict\n",
    "    .rolling({\"nlat\": nlats, \"nlon\": nlons})\n",
    "    .construct({\"nlat\": \"nlat_input\", \"nlon\": \"nlon_input\"})[{'nlat':nlat_range, 'nlon':nlon_range}]\n",
    "    .stack({\"input_batch\": (\"nlat\", \"nlon\")}, create_index=False)\n",
    "    .rename_dims({'nlat_input':'nlat', 'nlon_input':'nlon'})\n",
    "    .transpose('input_batch',...)\n",
    "    # .chunk({'input_batch':32, 'nlat':nlats, 'nlon':nlons})\n",
    "    .dropna('input_batch')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062e7321-9b61-4c41-9d4c-f722e4bfb960",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('models/'+ sc.name, custom_objects={'Grid_MAE':Grid_MAE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad0238-dee1-4a62-84f8-350da00232e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_conv  = [batch_test[x]      for x in sc.conv_var]\n",
    "predict_input = [batch_test[x][sub] for x in sc.input_var]\n",
    "predict_conv  = xr.merge(predict_conv ).to_array('var').transpose(...,'var')\n",
    "predict_input = xr.merge(predict_input).to_array('var').transpose(...,'var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0c4b9c-3cc8-46da-9bec-2e50d1777407",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_target = model.predict([predict_conv, predict_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e889207-caa7-48ac-801e-639dd34d5793",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16edc192-740c-492d-8b7e-5184451fa298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_target = predict(ds_predict, sc5, conv_dims, conv_kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b09d6e-cfd5-4eac-ba4d-b9a9ffba8553",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f119d206-263b-4fc0-b7ef-ab9162032154",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prediction Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0836bf2-4510-4a6d-8b9e-d82c4260e5fa",
   "metadata": {},
   "source": [
    "Now, let's take a look at the predicted surface currents and see how the model did. Notice that the predicted data can be retrieved fairly easily from our default setup. We only have to reshape them, with respect to the original dimensions and a halo that will be stripped off. This is because we chose to make the convolution kernel equal to the dimensions of the samples, which means the model will give results at individual points. \n",
    "\n",
    "However, the convolution kernal can be different, it's just that we will then have to use a more complex process to restructure our grid.\n",
    "\n",
    "Note also that if there were nans removed, we would have to keep track of how to map the unstructured model inputs back to the original grid and insert nans in the correct positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd357e7-439c-4765-a63c-281f8e0398e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "U = ds_predict['U']\n",
    "V = ds_predict['V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5fe083-a070-49ef-ac14-e1655dab617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "U_pred = predict_target[:,0,0,0].reshape(545, 345)\n",
    "V_pred = predict_target[:,0,0,1].reshape(545, 345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057c72f-7e9c-4cd9-bd97-6fd0183e9324",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.pcolormesh(U, cmap='RdBu_r')\n",
    "plt.clim([-100, 100])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dde0b7-b414-4635-819c-3344f9abc924",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.pcolormesh(U_pred, cmap='RdBu_r')\n",
    "plt.clim([-100, 100])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffee3bbb-44bf-448d-a2d6-362534b06b98",
   "metadata": {},
   "source": [
    "We can see that they look very similar, but to get a better idea of what our errors look like, we can subtract them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a723fe-f2d3-4fd2-a6a6-5c953037863b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.pcolormesh(U_pred - U[3:-2,3:-2], cmap='RdBu_r') # double-check U indexing\n",
    "plt.clim([-100, 100])\n",
    "plt.colorbar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
